{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# FUNCTION TO WRITE UNIQUE VALUES #\n",
    "###################################\n",
    "\n",
    "# Function to count frquency of values in the rows of a dataset\n",
    "# INPUT:\n",
    "# df_temp [pyspark-dataframe] Dataframe on which operation needs to be performed\n",
    "# rowname [string] name of the string whose values need to counted\n",
    "# filename [string] name of the file which will store the result\n",
    "# OUTPUT:\n",
    "# CSV file with results of the operation\n",
    "def unique_writer(df_temp, rowname, filename):\n",
    "    df_unique = df_temp.groupby(rowname).count().sort(col(\"count\").desc())\n",
    "    df_unique.toPandas().to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# FUNCTION TO COUNT UNWANTED VALUES IN EVERY COLUMN #\n",
    "#####################################################\n",
    "\n",
    "# INPUT :\n",
    "# dataframe_name [string] name of the the dataframe on which operation needs to be preformed\n",
    "# file_name [string] name of the the file in which results need to be stored in csv format\n",
    "# OUTPUT:\n",
    "# CSV file with output of the operation\n",
    "def count_unwanted(dataframe_address, file_name):\n",
    "\n",
    "    df_temp = spark.read.csv(dataframe_address, header=True)\n",
    "\n",
    "    # unique_count = []\n",
    "    nan_count = []\n",
    "    null_count = []\n",
    "    q_count = []\n",
    "    zero_count = []\n",
    "    column_index = []\n",
    "\n",
    "    for i in df_temp.columns:\n",
    "        column_index.append(i)\n",
    "        # unique_count.append(df_temp.select(i).distinct().count())\n",
    "        nan_count.append(df_temp.where(df_temp[i].isNull()).count())\n",
    "        null_count.append(df_temp.filter(df_temp[i] == 'NaN').count())\n",
    "        q_count.append(df_temp.filter(df_temp[i] == '?').count())\n",
    "        zero_count.append(df_temp.filter(df_temp[i] == 0).count())\n",
    "\n",
    "    # unwanted_data = {\"unique\":unique_count, \"nan_count\": nan_count, \"null_count\": null_count, \"q_count\": q_count, \"zero_count\": zero_count}\n",
    "    unwanted_data = {\"nan_count\": nan_count, \"null_count\": null_count, \"q_count\": q_count, \"zero_count\": zero_count}\n",
    "\n",
    "    pd.DataFrame(unwanted_data, index=column_index).to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN BODY OF THE PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries and functionality\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # Creating Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Understanding Enteries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1) allergies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark.read.csv(\"dataset_project_1/allergies.csv\", header=True)\n",
    "\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/allergies_uvc1.csv\") # Unique Values in the the DESCRIPTION column\n",
    "unique_writer(df_temp, \"PATIENT\", \"eda/understanding_entries/allergies_uvc2.csv\") # Unique Values in the patient column\n",
    "\n",
    "# Determining if there are patients with multiple ongoing allergies\n",
    "df_temp = df_temp.where(df_temp[\"STOP\"].isNull())\n",
    "unique_writer(df_temp, \"PATIENT\", \"eda/understanding_entries/allergies_nuvc1.csv\")\n",
    "\n",
    "# Unwanted Data\n",
    "count_unwanted(\"dataset_project_1/allergies.csv\", \"eda/understanding_entries/allergies_unwanted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                 | **Operation**                                                                     |\n",
    "|:-------------------------|:----------------------------------------------------------------------------------|\n",
    "| allergies_uvc1.csv       | Unique Values Count on \"DESCRIPTION\"                                              |\n",
    "| allergies_uvc2.csv       | Unique Values Count on \"PATIENT\"                                                  |\n",
    "| allergies_nuvc1.csv      | Unique Values Count on \"PATIENT\" on dataframe filtered by null   values in \"STOP\" |\n",
    "| allergies_unwanted.csv   | Count of unwanted data in the dataframe                                           |\n",
    "\n",
    "\n",
    "• **Allergy to mould** is the most frequent entry and Allergy to soya is the least common entry (see\n",
    "<i>allergies_uvc1.csv</i>)\n",
    "<br> <br>\n",
    "• It is possible for a single patient id to be listed against multiple enteries in allergies.csv, (see <i>allergies uvc2.csv</i>) however is it possible for a patient to have multiple allergies remain to be seen. \n",
    "<br> <br>\n",
    "• It is possible for a patient to have multiple ongoing allergies (see <i>allergies_nuvc1.csv</i> )\n",
    "<br> <br>\n",
    "• \"STOP\" column in allergies.csv have some null values, but this cannot be attributed to unwanted or\n",
    "missing data, it represents entries with ongoing allergies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2) careplans.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREPLANS #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/careplans.csv\", header=True)\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"PATIENT\", \"eda/understanding_entries/careplans_uvc1.csv\") # PATIENT Column\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/careplans_uvc2.csv\") # DESCRIPTION column\n",
    "unique_writer(df_temp, \"REASONDESCRIPTION\", \"eda/understanding_entries/careplans_uvc3.csv\") # REASONDESCRIPTION column\n",
    "\n",
    "# Unique Values Count on Datafarem filtered by NaN values\n",
    "df_temp1 = df_temp.where(df_temp[\"STOP\"].isNull()) # Filtering the data by null values in STOP column\n",
    "unique_writer(df_temp1, \"PATIENT\", \"eda/understanding_entries/careplans_nuvc1.csv\") # unique values count on PATIENT column\n",
    "\n",
    "df_temp2 = df_temp.where(df_temp[\"REASONDESCRIPTION\"].isNull()) # Filtering the data by NaN values in REASONDESCRIPTION column\n",
    "unique_writer(df_temp2, \"DESCRIPTION\", \"eda/understanding_entries/careplans_nuvc2.csv\") # Unique values on the DESCRITION column\n",
    "\n",
    "\n",
    "count_unwanted(\"dataset_project_1/careplans.csv\", \"eda/understanding_entries/careplans_unwanted.csv\") # Unwanted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**            | Operation**                                                                                         |\n",
    "|:-----------------------|:-------------------------------------------------------------------------------------------------|\n",
    "| careplans_uvc1.csv     | Unique Values Count on \"PATIENT\"                                                                 |\n",
    "| careplans_uvc2.csv     | Unique Values Count on \"DESCRIPTION\"                                                             |\n",
    "| careplans_uvc3.csv     | Unique Values Count on \"REASONDESCRIPTION\"                                                       |\n",
    "| careplans_nuvc1.csv    | Unique Values Count on \"PATIENT\" on dataframe filtered by null values in \"STOP\"                  |\n",
    "| careplans_nuvc2.csv    | Unique Values Count on \"DESCRIPTION\" on dataframe filtered by null values in \"REASONDESCRIPTION\" |\n",
    "| careplans_unwanted.csv | Count of unwanted data in the dataframe                                                          |\n",
    "\n",
    "• It is possible for a patient to have multiple careplans (see <i>careplans_uvc1.csv</i>), however further investigation is required to check if it is possible for a patient to have multiple ongoing careplans.\n",
    "<br> <br>\n",
    "• To study the most common reasons careplans are used for, it was deterimined to perform a unique Values\n",
    "count on the columns named \"DESCRIPTION\" and \"REASONDESCRIPTION\" of the careplans.csv. \n",
    "<br> <br>\n",
    "Perfomring a unique values count on either one of the column will not be sufficientcient as single value in the\n",
    "\"DESCRIPTION\" colummn could be listed against multiple entries in the \"REASONDESCRIPTION\"\n",
    "column. \n",
    "<br> <br>\n",
    "• Careplans are most commonly used for **respiratory therapy** (see <i>careplans_uvc2.csv</i>) \n",
    "<br> <br>\n",
    "• Careplans are most commonly used for treatment of **Acute Bronchitis(Disorder)** (see <i>careplans_uvc3.csv</i>)\n",
    "which is a respiratory disorder thus justifying the fact that Respiratory Therepy is the most commonly\n",
    "occuring value in the \"DESCRIPTION\" column. \n",
    "<br> <br>\n",
    "The \"STOP\" column contains NaN values but it could be attributed to ongoing careplans and cannot\n",
    "be considered as unwanted or bad data. The \"REASONDECSRIPTION\" column also contains NaN\n",
    "values it represents the use of careplans for non-medical reasons. Thus we can say that careplans.csv\n",
    "does not contains any unwanted data (see <i>careplans_unwanted.csv</i>). \n",
    "<br> <br>\n",
    "• For non-medical reason (i.e. when \"REASONDESCRIPTION\" has a **NaN value**) the careplans are most\n",
    "commonly used for **Self-care interventions (procedure)** (see <i>careplans_nuvc2.csv</i>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3) conditions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDITIONS #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/conditions.csv\", header=True) # Reading the dataframe\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/conditions_uvc1.csv\") # DESCRIPTION Column\n",
    "\n",
    "# Unique Values Count on null filtered Dataframe\n",
    "df_temp1 = df_temp.where(df_temp[\"STOP\"].isNull()) # Filtering by NaN values in STOP ccolumn\n",
    "unique_writer(df_temp1, \"DESCRIPTION\", \"eda/understanding_entries/conditions_nuvc1.csv\") # performing unique values count - The conditions in this file will be called \"Assumed chronic\"\n",
    "\n",
    "# Dtermining if any of the conditions which do not have a stop date has stop dates in any of the enteries\n",
    "chronic_conditions = set(df_temp1.toPandas()[\"DESCRIPTION\"].tolist()) # Set of all the conditions which do not have a STOP date\n",
    "df_temp2 = df_temp.where(df_temp[\"DESCRIPTION\"].isin(chronic_conditions)) # Filtering the dataframe with the above set\n",
    "df_temp3 = df_temp2.where(df_temp2[\"STOP\"].isNotNull()) # Filtering the dataset again to have non Null Values\n",
    "unique_writer(df_temp3, \"DESCRIPTION\", \"eda/understanding_entries/conditions_psudo_chroninc.csv\") # Writing the results to a file\n",
    "\n",
    "# NOTE: Psudo Chronic Conditions is self coined term which represents conditions which were classsified as chroninic(i.e.\n",
    "# having no stop date ) in some enteries but actually have STOP dates in some other enteries.\n",
    "\n",
    "# Making a list of chronic situations\n",
    "df_temp1 = spark.read.csv(\"eda/understanding_entries/conditions_nuvc1.csv\", header=True) # File with conditions from enteris which have no STOP date\n",
    "df_temp2 = spark.read.csv(\"eda/understanding_entries/conditions_psudo_chroninc.csv\", header=True) # File with enteries with psudo chronic conditions\n",
    "assumed_chronic = set(df_temp1.toPandas()[\"DESCRIPTION\"].tolist()) # set of conditions which have no STOP dates\n",
    "psudo_chronic = set(df_temp2.toPandas()[\"DESCRIPTION\"].tolist()) # set of psudo chronic conditions\n",
    "real_chronic = assumed_chronic - psudo_chronic # chronic conditions\n",
    "df_chronic = pd.Series(list(chronic_conditions)) # Crearing a dataframe of chronic conditions\n",
    "df_chronic.to_csv(\"preprocessed/conditions_chronic.csv\", index=False, header=False) # writing chronic conditions to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                     | **Operation**                                                                                                                         |\n",
    "|:-----------------------------|:--------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| conditions_uvc1.csv          | Unique Values Count on \"DESCRIPTION\"                                                                                                  |\n",
    "| conditions_nuvc1.csv         | Unique Values Count on \"DESCRIPTION\"  on dataframe filtered by null values in \"STOP\"                                                  |\n",
    "| conditions_psudo_chronic.csv | List of psudo chronic conditions i.e. conditions which have no \"STOP\" values in some cases but do have a \"STOP\" value somewhere else. |\n",
    "\n",
    "• **Viral Sinusitis (disorder)** was the most common condition (see <i>conditions_uvc1.csv</i>)\n",
    "<br> <br>\n",
    "• It was observed that the \"STOP\" column in the <i>careplans.csv</i> had some **NaN values**, thus giving the impression that certain conditions could be chronic in nature thus the dataset was filtered by **NaN values** in the \"STOP\" column and unique values count was performed on the \"DESCRIPTION\" column, the results of this operation are stored in <i>condition_nuvc1.csv</i>. However this approach has a major drawback,\n",
    "some conditions which do not have a stop date in certain cases do have stop dates in some other cases.\n",
    "<br> <br>\n",
    "• In order to check weather a condition is trully chronic in nature or not, further investigation was required. The dataset was checked to see if any of the conditions in the <i>conditions_nuvc1.csv </i> has stop\n",
    "dates anywhere in the dataset. The enteries pertaining to this investigation are recoded in <i>conditions_psudo_chronic.csv</i>. \n",
    "<br> <br>\n",
    "• To get the list of all chronic conditions, a difference operation was performed between the list of conditions in <i>conditions_nuvc1.csv </i> and conditions psudo chronic.csv. The resultant conditions were recorded in <i>conditions_chronic.csv</i> stored in the preprocessed folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4) encounters.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCOUNTER #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/encounters.csv\", header=True)\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"REASONDESCRIPTION\", \"eda/understanding_entries/encounters_uvc1.csv\") # REASONDESCRIPTION column\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/encounters_uvc2.csv\") # DESCRIPTION column\n",
    "unique_writer(df_temp, \"ENCOUNTERCLASS\", \"eda/understanding_entries/encounters_uvc3.csv\") #  ENCOUNTERCLASS column\n",
    "\n",
    "count_unwanted(\"dataset_project_1/encounters.csv\", \"eda/understanding_entries/encounters_unwanted.csv\") # Unwanted Data\n",
    "\n",
    "# Unique Values Count on Dataframe filtered by NaN values\n",
    "df_temp1 = df_temp.where(df_temp[\"REASONDESCRIPTION\"].isNull()) # Filtering the dataset by NaN values in REASONDECRIPTION column\n",
    "unique_writer(df_temp1, \"DESCRIPTION\", \"eda/understanding_entries/encounters_nuvc1.csv\") # Unique Value Count on DESCRIPTION column\n",
    "unique_writer(df_temp1, \"ENCOUNTERCLASS\", \"eda/understanding_entries/encounters_nuvc2.csv\") # Unique Value Count on ENCOUNTERCLASS column\n",
    "\n",
    "# Cross Tabulation operations\n",
    "df_temp.crosstab(\"REASONDESCRIPTION\", \"ENCOUNTERCLASS\").toPandas().to_csv(\"eda/understanding_entries/encounters_ct1.csv\", index=False) # REASONDESCRIPTION and ENCOUNTERCLASS\n",
    "df_temp.crosstab(\"DESCRIPTION\", \"ENCOUNTERCLASS\").toPandas().to_csv(\"eda/understanding_entries/encounters_ct2.csv\", index=False) # DESCRIPTION and ENCOUNTERCLASS\n",
    "df_temp.crosstab(\"DESCRIPTION\", \"REASONDESCRIPTION\").toPandas().to_csv(\"eda/understanding_entries/encounters_ct3.csv\", index=False) # DESCRIPTION and REASONDECSRIPTION\n",
    "\n",
    "# Cross Tabulation operation on Dataframe filtered by NaN operations\n",
    "# df_temp1 = df_temp.where(df_temp[\"REASONDESCRIPTION\"].isNull())\n",
    "# df_temp1.crosstab(\"DESCRIPTION\", \"ENCOUNTERCLASS\").toPandas().to_csv(\"eda/encounters_REASONDESCRIPTION_null_crosstab.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                 | **Operation**                                                                      |\n",
    "|:-------------------------|:-----------------------------------------------------------------------------------|\n",
    "| encounters_uvc1.csv      | Unique Values Count on \"REASONDESCRIPTION\"                                         |\n",
    "| encounters_uvc2.csv      | Unique Values Count on \"DESCRIPTION\"                                               |\n",
    "| encounters_uvc3.csv      | Unique Values Count on \"ENCOUNTERCLASS\"                                            |\n",
    "| encounters_nuvc1.csv     | Unique Values Count on \"DESCRIPTION\" filtered by NaN values \"REASONDESCRIPTION\"    |\n",
    "| encounters_nuvc2.csv     | Unique Values Count on \"ENCOUNTERCLASS\" filtered by NaN values \"REASONDESCRIPTION\" |\n",
    "| encounters_ct1.csv       | Cross Tabulation operation between \"REASONDESCRIPTION\" and \"ENCOUNTERCLASS\"        |\n",
    "| encounters_ct2.csv       | Cross Tabulation operation between \"DESCRIPTION\" and \"ENCOUNTERCLASS\"              |\n",
    "| encounters_ct3.csv       | Cross Tabulation between \"DESCRIPTION\" and \"REASONDESCRIPTION\" column              |\n",
    "| encounters_unwanted. csv | Count of Unwanted data on the dataframe                                            |\n",
    "\n",
    "\n",
    "• Most encounters happen because of non-medical reasons i.e. **NaN values** and the second most common\n",
    "cause of encounters is **Normal Pregenancy** (see <i>encounters_uvc1.csv</i>).\n",
    "<br> <br>\n",
    "• As discussd above the most encounters are due to non-medical reasons, a further analysis reveals that\n",
    "most encounters happen can be classided as **Well child visit (procedure)** (see <i>encounters_uvc2.csv</i>).\n",
    "<br><br>\n",
    "• For most of the encounters the patient was in an ambulatory state i.e. the patient is not bed ridden\n",
    "and can walk (see <i>encounters_uvc3.csv</i>).\n",
    "<br> <br>\n",
    "• <i>encounters.csv</i> was also analyzed for unwanted data and the output was stored in <i>encounters_unwanted.csv</i>. It was observed that only \"REASONDESCRIPTION\" had **NaN values** and \"PAYER COVERAGE\"\n",
    "had some zeros. It is possible that a patient had no insurance coverage at all hence the zeros in the \"PAYER COVERAGE\" column cannot be treated as unwanted values values however the null values in \"REASONDESCRIPTION\" column need to be investigated further. Thus at this point it can't be stated with absolute certainty that <i>encounters.csv</i> does not have any unwanted data (see <i>encounters_unwanted.csv</i>).\n",
    "<br> <br>\n",
    "• For encounters which are not because any major medical purpouse the most common enounters are for **Well child visit (procedure)** (see <i>encounters_nuvc2.csv</i>) . After comparing the data in <i>encounters_uvc2.csv</i> and <i>encounters_nuvc1.csv</i> we can say that all the encounters of the type **Well child visit(procedure)** are for non-medical reasons.\n",
    "<br><br>\n",
    "• It was also observed that most of the encounters for non-medical reason are most commonly of the type **wellness** (see <i>encounters_nuvc2.csv</i>)\n",
    "<br><br>\n",
    "• On a closer observation it was noticed that a single entry in \"DESCRIPTION\" column could be listed against multiple entries in \"REASONDESCRIPTION\" and \"ENCOUNTERCLASS\". Thus several cross tabulation operations were performed to understand the distribution of the data across multiple categories.\n",
    "<br> <br>\n",
    "• Most of the entries in \"REASONDESCRIPTION\" and \"DESCRIPTION\" could be listed across multiple entries of \"ENCOULNTERCLASS\" (see <i>encounters_ct1.csv</i> and <i>encounters ct2.csv</i> ).\n",
    "<br> <br>\n",
    "• It must also be noted that several entries in the \"DESCRIPTION\" column could be listed against more than one entry in the \"REASONDESCRIPTION\" column (see <i>encounters_ct3.csv</i>) .\n",
    "<br> <br>\n",
    "• The \"ENCOUTERCLASS\" column has several unique values, we are particularly interested in the\n",
    "encounters where \"ENCOUTERCLASS\" is **emergency** and **urgentcare**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5) imaging_studies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGING STUDIES #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/imaging_studies.csv\", header=True)\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"BODYSITE_DESCRIPTION\", \"eda/understanding_entries/imaging_studies_uvc1.csv\") # BODYSITE_DESCRIPTION\n",
    "# unique_writer(df_temp, \"MODALITY_DESCRIPTION\", \"eda/imaging_studies_MODALITY_DESCRIPTION.csv\") # MODATLITY_DESCRIPTION\n",
    "unique_writer(df_temp, \"SOP_DESCRIPTION\", \"eda/understanding_entries/imaging_studies_uvc2.csv\") # SOP_DESCRIPTION\n",
    "\n",
    "count_unwanted(\"dataset_project_1/imaging_studies.csv\", \"eda/understanding_entries/imaging_studies_unwanted.csv\")\n",
    "\n",
    "# Cross Tabulation Operation\n",
    "# df_temp.crosstab(\"BODYSITE_DESCRIPTION\", \"MODALITY_DESCRIPTION\").toPandas().to_csv(\"eda/imaging_studies_crosstab_BODYSITE_DESCRIPTION_MODALITY_DESCRIPTION.csv\", index=False)\n",
    "df_temp.crosstab(\"BODYSITE_DESCRIPTION\", \"SOP_DESCRIPTION\").toPandas().to_csv(\"eda/understanding_entries/imaging_studies_ct1.csv\", index=False) # BODYSITE_DESCRIPTION and SOP_DESCRIPTION\n",
    "df_temp.crosstab(\"MODALITY_DESCRIPTION\", \"SOP_DESCRIPTION\").toPandas().to_csv(\"eda/understanding_entries/imaging_studies_ct2.csv\", index=False) # MODALITY_DESCRIPTION and SOP_DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                     | **Operation**                                                              |\n",
    "|:-----------------------------|:---------------------------------------------------------------------------|\n",
    "| imaging_studies_uvc1.csv     | Unique Values Count on \"BODYSTIE_DECSRIPTION\"                              |\n",
    "| imaging_studies_uvc2.csv     | Unique Values Count on \"MODALITY_DESCRIPTION\"                              |\n",
    "| imaging_studies_ct1.csv      | Cross Tabulation operation on \"BODYSITE_DESCRIPTION\" and \"SOP_DESCRIPTION\" |\n",
    "| imaging_studies_ct2.csv      | Cross Tabulation operation on \"MODALITY_DESCRIPTION\" and \"SOP_DESCRIPTION\" |\n",
    "| imaging_studies_unwanted.csv | Count of Unwanted Data in Dataframe                                        |\n",
    "\n",
    "• **Thoracic Structutre (Body Study)** is the most examined body structure (see <i>imaging_studies_uvc1.csv</i>)\n",
    "<br><br>\n",
    "• **Digital X-ray** is the most commonly performed procedure (see imaging <i>studies_uvc2.csv</i>).\n",
    "<br><br>\n",
    "• It can be noted that multiple tests could be performed on a single body part.\n",
    "<br><br>\n",
    "• **Thoracic structure (body structure)** is the only value in \"BODYSITE_DECSRIPTION\" that is listed against multiple values in the \"SOP DESCRIPTION\" column (see <i>imaging_studies_ct1.csv</i>). It must also be noted that \"BODYSITE_DESCRIPTION\" also have values **Thoracic structure** and **thoracic** which are listed against a single value in \"SOP_DESCRIPTION\" column. However if these represent different body site or are just a different values for the the same body site is a topic of further analysis. All ohter values in \"BODYSITE_DESCRIPTION\" column are listed against a single value in \"SOP_DESCRIPTION\" column.\n",
    "<br><br>\n",
    "• <i>imaging_studies.csv</i> does not contains any unwanted data (see <i>imaging_studies_unwanted.csv</i>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6) immunizations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMMUNIZATION #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/immunizations.csv\", header=True)\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/imunization_uvc1.csv\")\n",
    "\n",
    "count_unwanted(\"dataset_project_1/immunizations.csv\", \"eda/understanding_entries/immunization_unwanted.csv\") # Unwanted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| File                       | Operation                                   |\n",
    "|:---------------------------|:--------------------------------------------|\n",
    "| immunization_uvc1.csv      | Unique Values Count on \"DESCRIPTION\" column |\n",
    "| immunization_unwanted.csv  | Count of Unwanted Data on Dataframe         |\n",
    "\n",
    "• **Inuenza seasonal injectable preservative free** was the most frequently occurring entry (see <i>immunization_uvc1.csv</i>).\n",
    "<br><br>\n",
    "• <i>immunization.csv</i> contains no unwanted data (see <i>immunization_unwanted.csv</i>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.7) medications.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEDICATION #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/medications.csv\", header=True)\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/medications_uvc1.csv\") # DESCRIPTION\n",
    "unique_writer(df_temp, \"REASONDESCRIPTION\", \"eda/understanding_entries/medications_uvc2.csv\") # REASONDECRIPTION\n",
    "\n",
    "count_unwanted(\"dataset_project_1/medications.csv\", \"eda/understanding_entries/medications_unwanted.csv\")\n",
    "\n",
    "# Cross Tabulation Operation\n",
    "df_temp.crosstab(\"DESCRIPTION\", \"REASONDESCRIPTION\").toPandas().to_csv(\"eda/understanding_entries/medications_ct1.csv\", index=False)\n",
    "\n",
    "# Unique Values Count on Dataframe filtered by null values\n",
    "df_temp1 = df_temp.where(df_temp[\"REASONDESCRIPTION\"].isNull()) # filtering by NaN values in REASONDESCRIPTION column\n",
    "unique_writer(df_temp1, \"DESCRIPTION\", \"eda/understanding_entries/medications_nuvc1.csv\") #unique values count on DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                 | **Operation**                                                                                   |\n",
    "|:-------------------------|:------------------------------------------------------------------------------------------------|\n",
    "| medications_uvc1.csv     | Unique Values Count on \"DESCRIPTION\" column                                                     |\n",
    "| medications_uvc2.csv     | Unique Values Count on \"REASONDESCRIPTION\" column                                               |\n",
    "| medications_ct1.csv      | Cross Tabulation operation on \"DESCRIPTION\" and \"REASONDESCRIPTION\"                             |\n",
    "| medications_nuvc1.csv    | Unique Values Count on \"DESCRIPTION\" on dataframe filtered by NaN values in \"REASONDESCRIPTION\" |\n",
    "| medications_unwanted.csv | Count of Unwanted data in the dataframe                                                         |\n",
    "\n",
    "• **Hydrochlorothiazide 25 MG Oral Tablet** is the most common medication (see <i>medication_uvc1.csv</i>).\n",
    "<br><br>\n",
    "• Most medications are prescribed for minor medical reasons i.e. \"REASONDESCRIPTION\" having a\n",
    "**NaN value**, the second most common reason is **Hypertension** (see <i>medications_uvc2.csv</i>).\n",
    "<br><br>\n",
    "• In the absence on of any major medical reason i.e. \"REASONDESCRIPTION\" column has **NaN value**\n",
    "the most frequent entry in the data base is **Nitroglycerin 0.4 MG/ACTUAT Mucosal Spray** (see\n",
    "<i>medication_nuvc1.csv</i>)\n",
    "<br><br>\n",
    "• It was observed that \"REASONCODE\" contains some **NaN values**, some values in the \"PAYER\n",
    "COVERAGE\" and \"TOTAL COST\" columns are also **0**, although it is possible for a patient to have no\n",
    "health coverage, the possibility of \"TOTAL COST\" column having **0** might point towards the presence\n",
    "is any bad data in the medication.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.8) observations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/observations.csv\", header=True)\n",
    "\n",
    "# Unique Value Counts\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/observation_DESCRIPTION.csv\")\n",
    "\n",
    "count_unwanted(\"dataset_project_1/observations.csv\", \"eda/understanding_entries/observations_unwanted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                  |**Operations**                               |\n",
    "|:--------------------------|:--------------------------------------------|\n",
    "| observations_uvc.csv      | Unique Values Count on \"DESCRIPTION\" column |\n",
    "| observations_unwanted.csv | Count of Unwanted Data in the Dataframe     |\n",
    "\n",
    "• **Pain severity - 0-10 verbal numeric rating \\[SCORE\\]- Reported** is the most commonly performed\n",
    "procedure.\n",
    "<br><br>\n",
    "• Based on the nature of the data it is difficult to draw any meaningful conclusion, thus a further analysis\n",
    "is required to draw meaningful insights.\n",
    "<br><br>\n",
    "• Based on an initial analysis it was observed that \"ENCOUNTER\" and \"UNITS\" column has some **NaN values** and some values in the \"VALUE\" column are also zero. Thus based on an initial analysis it can be stated that <i>observations.csv</i> does have some unwanted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.9) procedures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCEDURES #\n",
    "\n",
    "df_temp = spark.read.csv(\"dataset_project_1/procedures.csv\", header=True)\n",
    "\n",
    "# Unique Values Count\n",
    "unique_writer(df_temp, \"DESCRIPTION\", \"eda/understanding_entries/procedures_uvc1.csv\") # DESCRIPTION column\n",
    "unique_writer(df_temp, \"REASONDESCRIPTION\", \"eda/understanding_entries/procedures_uvc2.csv\") # REASONDESCRIPTION column\n",
    "\n",
    "count_unwanted(\"dataset_project_1/procedures.csv\", \"eda/understanding_entries/procedures_unwabted.csv\") # Unwanted Data\n",
    "\n",
    "# Unique Values Count on Dataframe filtered by Null Values\n",
    "df_temp1 = df_temp.where(df_temp[\"REASONDESCRIPTION\"].isNull()) # Filtering by null values in REASONDESCRIPTION\n",
    "unique_writer(df_temp1, \"DESCRIPTION\", \"eda/understanding_entries/procedures_nuvc1.csv\") # counting unique values in DESCRIPTION column\n",
    "\n",
    "# Cross Tabulation Operation\n",
    "df_temp.crosstab(\"DESCRIPTION\", \"REASONDESCRIPTION\").toPandas().to_csv(\"eda/understanding_entries/procedures_ct1.csv\", index=False) # DESCRIPTION and REASONDESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**                | **Operation**                                                                                   |\n",
    "|:------------------------|:------------------------------------------------------------------------------------------------|\n",
    "| procedures_uvc1.csv     | Unique Values Count on \"DESCRIPTION\" column                                                     |\n",
    "| procedures_uvc2.csv     | Unique Values Count on \"REASONDESCRIPTION\" column                                               |\n",
    "| procedures_ct1.csv      | Cross Tabulation operation on \"DESCRIPTION\" and \"REASONDESCRIPTION\"                             |\n",
    "| procedures_nuvc11.csv   | Unique Values Count on \"DESCRIPTION\" on dataframe filtered by NaN Values in \"REASONDESCRIPTION\" |\n",
    "| procedures_unwanted.csv | Count of unwanted data on dataframe                                                             |\n",
    "\n",
    "\n",
    "• **Medication Reconciliation (procedure)** is the most frequently performed procedure (see <i>procedures_uvc1.csv</i>).\n",
    "<br><br>\n",
    "• Most of the procedures are performed due to **Normal Pregnenecy** (see <i>procedures_uvc2.csv</i>)\n",
    "<br><br>\n",
    "• It was observed that \"REASONDESCRIPTION\" does contain some **NaN Values** (see <i>proceduresnuvc1.csv</i>).\n",
    "However these cannot be regarded as missing values as this represents the cases where there is no major\n",
    "medical reason.\n",
    "<br><br>\n",
    "• Even in the absence of any major medical reason it was observed that **Medication Reconciliation\n",
    "(procedure)** was most frequently performed procedure(see <i>procedures_nuvc1.csv</i>) however it must be\n",
    "observed that in the absence of any major medical reason i.e. \"REASONDESCRIPTION\" column has\n",
    "a NaN Value.\n",
    "<br><br>\n",
    "• On a closer obervation it could be observed that a procedure could be performed for multiple medical\n",
    "reasons (see <i>procedures_ct1.csv</i> )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
